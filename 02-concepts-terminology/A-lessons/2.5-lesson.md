## **Lesson 2.5: Storage Systems - Warehouses vs Data Lakes vs Lakehouses**

### **The Storage Challenge:**
"Different data needs different treatment - raw storage, structured querying, or both."

---

### **1. Data Warehouse**

**What it is:** Structured storage optimized for SQL analytics

**Think of it as:** A well-organized library
- Books sorted by category
- Easy to find what you need
- Everything cataloged

**Characteristics:**
- Stores **structured data only** (tables with schemas)
- Optimized for **fast SQL queries**
- Data is **clean and processed**
- **Expensive** storage
- Follows **ETL** - data is transformed BEFORE loading

**Examples:** Snowflake, Google BigQuery, Amazon Redshift

**What goes here:**
- Cleaned, analytics-ready data
- Sales transactions
- Customer records
- Product catalogs

**Who uses it:** Data Analysts, Business Intelligence teams

**Cost:** $$$

---

### **2. Data Lake**

**What it is:** An architectural pattern built on top of cheap object storage 
(e.g. S3, Azure Blob, MinIO) for storing ALL types of data

> **Note:** Object stores (S3, MinIO) are just the storage technology/foundation.
> A Data Lake is the concept/pattern built on top - it adds folder organization,
> processing engines (Spark, Hive) and intention to the raw storage.
> In practice, companies often confused the two by pointing to an S3 bucket
> and calling it a "data lake" - leading to unorganized "data swamps."

**Think of it as:** A giant warehouse
- Everything stored in organized sections/folders
- Raw, minimally processed
- Cheap to store
- Has processing tools on top for engineers/scientists to work with

**Characteristics:**
- Stores **all data types** (structured, semi-structured, unstructured)
- Data stored **as-is** (raw, not cleaned)
- Has some **folder/zone organization** (e.g. /raw/, /processed/)
- **Very cheap** storage
- Harder to query than a warehouse
- Follows **ELT** - data lands first, transformed later

**Examples:** 
- Object Store (foundation): Amazon S3, Azure Blob Storage, Google Cloud Storage, MinIO
- Data Lake (built on top): Azure Data Lake Storage (ADLS), AWS Lake Formation

**What goes here:**
- Log files
- Images and videos
- Raw JSON from APIs
- CSV dumps
- Backups
- Any raw, unprocessed data

**Who uses it:** Data Engineers (for processing), Data Scientists (for exploration)

**Cost:** $

---

### **3. Data Lakehouse**

**What it is:** The evolution of the Data Lake - adds Data Warehouse  capabilities on top of cheap lake storage, eliminating the need for two separate systems.

**Why it exists:** Traditional data lakes became "data swamps" - data was dumped but hard to manage, query, or trust. Companies also had to maintain BOTH a data lake AND a data warehouse,  meaning data was duplicated across two systems.
The Lakehouse solves both problems.

**Think of it as:** A smart, evolved data lake
- Still stores everything cheaply like a lake
- But now organized, governed and queryable like a warehouse
- One platform for everyone

**Characteristics:**
- Stores **all data types** (like a lake)
- Supports **fast SQL queries** (like a warehouse)
- **ACID transactions** - no partial writes or data corruption
- **Schema enforcement** - data quality controls
- **Time travel** - view data as it was at any point in time
- **Unified batch & streaming** processing
- Uses open table formats (Delta Lake, Apache Iceberg, Apache Hudi)
- Implements **Medallion Architecture** (Bronze → Silver → Gold)
- Follows **ELT** - raw data lands first, transformations happen across layers
- **Centralized governance** (e.g. Unity Catalog in Databricks)

**Examples:** Databricks (Delta Lake), Apache Iceberg + Spark, AWS Lake Formation + Iceberg

**How it works:**
```
Object Store - S3, MinIO (cheap raw storage)
   +
Open Table Format - Delta Lake/Iceberg (ACID, schema, time travel)
   +
Processing Engine - Spark (transforms data across Bronze → Silver → Gold)
   +
Governance Layer - Unity Catalog (access control, lineage, discovery)
   =
One unified platform for Engineers, Analysts and Scientists!
```

**Who uses it:** Modern data teams - Engineers, Analysts, Scientists, ML teams

**Cost:** $$

---

### **Comparison Table:**

| Feature | Data Warehouse | Data Lake | Data Lakehouse |
|---------|---------------|-----------|----------------|
| **Data types** | Structured only | All types | All types |
| **Schema** | Strict (schema-on-write) | Flexible (schema-on-read) | Flexible with governance |
| **Query speed** | Fast | Slow | Fast |
| **Cost** | High | Low | Medium |
| **Data quality** | High (curated) | Low (raw) | High (validated across layers) |
| **Users** | Analysts | Engineers, Scientists | Everyone |
| **Methodology** | ETL | ELT | ELT |
| **Transaction support** | Yes (ACID) | No | Yes (ACID) |
| **Examples** | Snowflake, BigQuery | S3 + Spark, ADLS | Databricks, Iceberg + Spark |

---

### **When to Use What:**

**Use Data Warehouse when:**
- Need fast SQL queries on clean structured data
- Business reporting and dashboards
- Example: Monthly sales reports, executive dashboards

**Use Data Lake when:**
- Storing large volumes of raw/unstructured data cheaply
- Data use case is unknown yet
- Example: Log archives, image/video backups, raw API dumps

**Use Data Lakehouse when:**
- Want both flexibility and warehouse-like performance
- Need to support diverse users (analysts + scientists + engineers)
- Want ONE platform instead of maintaining lake + warehouse separately
- Example: Unified analytics + ML platform

---

### **The Evolution:**
```
Object Store         →    Data Lake           →    Data Lakehouse
(just storage)            (storage +               (storage +
                          organization +            organization +
                          processing tools)         processing +
                                                    warehouse features +
                                                    governance)
```

---

### **Real-World Scenario:**

**Company:** Ride-sharing app (like Uber)

**Old approach (separate systems):**
```
Data Lake (S3 + Spark):               Data Warehouse (Snowflake):
- Raw GPS logs (millions/sec)         - Clean trip_history table
- Driver photos                       - driver_performance metrics  
- Trip receipts (PDFs)                - Monthly revenue reports
- App logs                            
→ Used by Engineers & Scientists      → Used by Analysts only

Problem: Data duplicated across both systems!
```

**Modern approach (Lakehouse):**
```
Lakehouse (Databricks on S3):

Bronze Layer:  Raw GPS logs, photos, receipts, app logs
               ↓
Silver Layer:  Cleaned trips, validated drivers, conformed transactions
               ↓
Gold Layer:    trip_history, driver_performance, revenue_reports,
               ML-ready datasets

→ Engineers, Scientists AND Analysts all use the same platform
→ No data duplication
→ One governance layer (Unity Catalog) controls all access
```