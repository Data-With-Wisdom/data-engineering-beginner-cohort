## **Lesson 2.9: Common Data Engineering Mistakes**

### **Mistake #1: No Data Validation**

**What happens:**
```
Pipeline runs successfully
   ↓
Loads 1 million rows
   ↓
Later discover: 500k rows are duplicates
   ↓
Analysts use wrong data for decisions
```

**Solution:** Add validation checks

---

### **Mistake #2: Hard-Coding Values**

**Bad:**
```python
df = pd.read_csv('/Users/ahmed/Desktop/data.csv')
```

**Why bad:** Won't work on teammate's computer or in production

**Good:**
```python
data_path = os.getenv('DATA_PATH')
df = pd.read_csv(data_path)
```

---

### **Mistake #3: No Error Handling**

**Bad:**
```python
data = fetch_from_api()
process(data)
```

**What happens:** API down → code crashes → pipeline stops

**Good:**
```python
try:
    data = fetch_from_api()
    process(data)
except Exception as e:
    log_error(e)
    send_alert("Pipeline failed!")
```

---

### **Mistake #4: Ignoring Data Freshness**

**Problem:** Using yesterday's data for real-time decisions

**Solution:** Monitor and alert on data age

---

### **Mistake #5: Poor Documentation**

**Bad:** Code with no comments, no README

**Result:** Teammate can't understand your pipeline 6 months later

**Good:** Clear README, inline comments, data dictionaries

---

## **Week 2 Checklist:**

By end of Week 2, you should be able to:
- ✅ Explain all 6 data lifecycle stages with examples
- ✅ Differentiate ETL, ELT, and Zero-ETL
- ✅ Identify structured vs semi-structured vs unstructured data
- ✅ Choose appropriate storage (warehouse/lake/lakehouse) for scenarios
- ✅ Understand primary/foreign keys
- ✅ Explain why data quality matters
- ✅ Describe what pipelines and orchestration do (conceptually)

This builds the vocabulary you need before diving into SQL and Python.