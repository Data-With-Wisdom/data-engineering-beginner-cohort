## **Lesson 1.3: How Data Flows in Companies**

### **The Data Lifecycle:**

```
1. GENERATION → 2. COLLECTION → 3. STORAGE → 4. PROCESSING → 5. ANALYSIS → 6. ARCHIVAL
```

Let's follow an **e-commerce order** through this lifecycle:

---

**1. GENERATION (Data is Created)**
- Customer browses website, clicks on products
- Adds sneakers to cart
- Completes purchase
- **Data generated:** user clicks, product views, purchase transaction

---

**2. COLLECTION (Data is Captured)**
- Website sends events to backend servers
- Payment gateway confirms transaction
- Inventory system records item sold
- **How it's collected:** API calls, event tracking (Google Analytics-like systems)

---

**3. STORAGE (Data is Saved)**
- Transaction saved in **PostgreSQL database** (for immediate operational needs)
- Raw event logs sent to **data lake** (Amazon S3 or MinIO) for long-term storage
- **Why multiple storage?** Different purposes (live operations vs historical analysis)

---

**4. PROCESSING (Data is Cleaned & Transformed)**
- Data Engineer runs nightly pipeline:
  - Removes duplicate orders
  - Standardizes addresses
  - Calculates total revenue per product
  - Joins order data with customer data
- **Output:** Clean, structured data in **data warehouse** (e.g., Snowflake, BigQuery)

---

**5. ANALYSIS (Data is Used)**
- **Data Analyst** queries warehouse: "What were top-selling products last month?"
- **Data Scientist** builds model: "Which customers are likely to return items?"
- **Business team** views dashboard: Real-time sales metrics

---

**6. ARCHIVAL (Old Data is Stored Long-Term)**
- Data older than 2 years moved to cheaper storage
- Kept for compliance (tax records, audits)
- Can be retrieved if needed

---

### **Visual Diagram (Provide this as an image/slide):**

```
[User Action] → [App Logs] → [Database] → [Data Lake] 
                                              ↓
                                        [ETL Pipeline]
                                              ↓
                                       [Data Warehouse]
                                              ↓
                            [Dashboards] [Reports] [ML Models]
```

---

### **Real-World Example: Netflix**

1. **GENERATION:** You watch "Squid Game" Episode 3, pause at 23:14
2. **COLLECTION:** Viewing event sent to Netflix servers
3. **STORAGE:** Event stored in distributed database (Apache Kafka topic)
4. **PROCESSING:** Hourly batch job calculates "Users who watched Episode 3 also liked..."
5. **ANALYSIS:** Recommendation algorithm uses this to personalize your homepage
6. **ARCHIVAL:** Viewing data older than 1 year archived for trend analysis

---